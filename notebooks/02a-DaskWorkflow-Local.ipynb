{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Workflow - Local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo notebooks 02a and 02b will show you the recommended way to work with Dask on a cluster.\n",
    "\n",
    "Requirements:\n",
    " - Cluster has been added and configured, as shown in notebook 01.\n",
    " - `idact` is installed on the cluster using pip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `idact` to path.\n",
    "\n",
    "These steps would not usually be required, but for the purpose of separating the demo from your live config, key and config paths are substituted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import bitmath\n",
    "import logging\n",
    "import subprocess\n",
    "from pprint import pprint\n",
    "\n",
    "# appending path is not necessary if idact was installed using pip\n",
    "def append_idact_path():\n",
    "    idact_path = os.path.realpath(os.path.join(os.getcwd(), '../'))\n",
    "    sys.path.append(idact_path)\n",
    "append_idact_path()\n",
    "\n",
    "# comment out the line below to use the default key location: ~/.ssh\n",
    "os.environ['IDACT_KEY_LOCATION'] = os.path.join(os.getcwd(), '../.notebook-ssh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cluster(pro.cyfronet.pl, 22, plggarstka, auth=AuthMethod.PUBLIC_KEY, key='E:\\\\shared\\\\uni\\\\eng-project\\\\notebooks\\\\../.notebook-ssh\\\\id_rsa_du', install_key=False, disable_sshd=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from idact import *\n",
    "\n",
    "load_environment('.idact-env')  # load_environment() would use the default path: ~/.idact.conf\n",
    "cluster = show_cluster(\"pro\")  # replace with your cluster name if necessary\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure it's properly configured. Replace the following with correct values for your cluster, or skip if already configured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_log_level(logging.INFO)\n",
    "#set_log_level(logging.DEBUG)\n",
    "cluster.config.setup_actions.jupyter = ['module load plgrid/tools/python-intel/3.6.2']\n",
    "cluster.config.setup_actions.dask = ['module load plgrid/tools/python-intel/3.6.2']\n",
    "cluster.config.scratch = '$SCRATCH'\n",
    "\n",
    "save_environment('.idact-env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get access node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Node(pro.cyfronet.pl:22, None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node = cluster.get_access_node()\n",
    "node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure authentication is set up correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plggarstka'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.run('whoami')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'login01.pro.cyfronet.pl'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.run('hostname')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allocate nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need nodes to deploy Jupyter Notebook and Dask on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-02 22:34:08 INFO: Creating the ssh directory.\n"
     ]
    }
   ],
   "source": [
    "nodes = cluster.allocate_nodes(nodes=2,\n",
    "                               cores=2,\n",
    "                               memory_per_node=bitmath.GiB(10),\n",
    "                               walltime=Walltime(minutes=20),\n",
    "                               native_args={\n",
    "                                   '--partition': 'plgrid-testing',\n",
    "                                   '--account': 'intdata'\n",
    "                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nodes([Node(NotAllocated),Node(NotAllocated)], SlurmAllocation(job_id=13969005))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until the nodes are allocated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nodes([Node(p0653:51136, 2018-11-02 21:54:16.293426+00:00),Node(p0657:56621, 2018-11-02 21:54:16.293426+00:00)], SlurmAllocation(job_id=13969005))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.wait()\n",
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will work from a remote Jupyter Notebook deployed on the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JupyterDeployment(8080 -> Node(p0653:51136, 2018-11-02 21:54:16.293426+00:00)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = nodes[0].deploy_notebook()\n",
    "nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the remote notebook in a new tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.open_in_browser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy Dask on the notebook, you will need access to nodes you allocated earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push the nodes to the cluster. You will be able to use it straigth away by calling `load_environment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-02 22:34:39 INFO: Pushing deployment: Nodes([Node(p0653:51136, 2018-11-02 21:54:16.293426+00:00),Node(p0657:56621, 2018-11-02 21:54:16.293426+00:00)], SlurmAllocation(job_id=13969005))\n"
     ]
    }
   ],
   "source": [
    "cluster.push_deployment(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is your first time working on the cluster from a remote notebook,\n",
    "you may want to push the current environment. Alternatively, just `add_cluster` and perform other configuration steps on the remote notebook, as demonstrated in demo notebook 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-02 22:34:56 INFO: Pushing the environment to cluster.\n",
      "2018-11-02 22:34:58 ERROR: Failure: Getting file from node pro.cyfronet.pl: /net/people/plggarstka/.idact.conf\n",
      "2018-11-02 22:34:58 ERROR: Failure: Deserializing the environment from cluster.\n",
      "2018-11-02 22:34:58 INFO: Remote environment is missing, current environment will be copied to cluster.\n"
     ]
    }
   ],
   "source": [
    "push_environment(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy next notebook to the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drag and drop `02b-DaskWorkflow-Remote.ipynb` to the notebook you previously opened in a new tab. Open it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow the instructions in notebook 02b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow instructions until you are referred back to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Dask diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask deployment synchronization is currently not implemented.\n",
    "\n",
    "To view Dask scheduler dashboard, you will need to open the tunnel manually.\n",
    "Copy the port number from client description displayed on the remote notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 48170  # copy port from other notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-02 22:37:49,951| ERROR   | Could not establish connection from ('127.0.0.1', 53777) to remote side of the tunnel\n",
      "2018-11-02 22:37:49,959| ERROR   | Exception: Error reading SSH protocol banner\n",
      "2018-11-02 22:37:50,049| ERROR   | Traceback (most recent call last):\n",
      "2018-11-02 22:37:50,050| ERROR   |   File \"E:\\Anaconda3\\envs\\idact-dev\\lib\\site-packages\\paramiko\\transport.py\", line 2044, in _check_banner\n",
      "2018-11-02 22:37:50,050| ERROR   |     buf = self.packetizer.readline(timeout)\n",
      "2018-11-02 22:37:50,051| ERROR   |   File \"E:\\Anaconda3\\envs\\idact-dev\\lib\\site-packages\\paramiko\\packet.py\", line 353, in readline\n",
      "2018-11-02 22:37:50,052| ERROR   |     buf += self._read_timeout(timeout)\n",
      "2018-11-02 22:37:50,052| ERROR   |   File \"E:\\Anaconda3\\envs\\idact-dev\\lib\\site-packages\\paramiko\\packet.py\", line 542, in _read_timeout\n",
      "2018-11-02 22:37:50,053| ERROR   |     raise EOFError()\n",
      "2018-11-02 22:37:50,054| ERROR   | EOFError\n",
      "2018-11-02 22:37:50,054| ERROR   | \n",
      "2018-11-02 22:37:50,055| ERROR   | During handling of the above exception, another exception occurred:\n",
      "2018-11-02 22:37:50,055| ERROR   | \n",
      "2018-11-02 22:37:50,056| ERROR   | Traceback (most recent call last):\n",
      "2018-11-02 22:37:50,057| ERROR   |   File \"E:\\Anaconda3\\envs\\idact-dev\\lib\\site-packages\\paramiko\\transport.py\", line 1893, in run\n",
      "2018-11-02 22:37:50,058| ERROR   |     self._check_banner()\n",
      "2018-11-02 22:37:50,058| ERROR   |   File \"E:\\Anaconda3\\envs\\idact-dev\\lib\\site-packages\\paramiko\\transport.py\", line 2049, in _check_banner\n",
      "2018-11-02 22:37:50,059| ERROR   |     'Error reading SSH protocol banner' + str(e)\n",
      "2018-11-02 22:37:50,059| ERROR   | paramiko.ssh_exception.SSHException: Error reading SSH protocol banner\n",
      "2018-11-02 22:37:50,060| ERROR   | \n",
      "2018-11-02 22:37:50,061| ERROR   | Could not connect to gateway 127.0.0.1:53777 : Error reading SSH protocol banner\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-02 22:37:50 WARNING: Exception: Could not establish session to SSH gateway, retry 1/3.\n"
     ]
    }
   ],
   "source": [
    "tunnel = nodes[0].tunnel(there=port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "webbrowser.open(\"http://localhost:{here}/status\".format(here=tunnel.here));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunnel.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor node resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While working with Dask, it may be useful to monitor resource usage on nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GiB(10.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].resources.memory_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].resources.cpu_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GiB(0.46537017822265625)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].resources.memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GiB(0.22327804565429688)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[1].resources.memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].resources.cpu_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[1].resources.cpu_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancel Dask deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cancel Dask deployment in the remote notebook, or just close it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancel other deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the nodes are still running, make sure you cancel their allocation to save CPU time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.running()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-02 22:39:37 INFO: Cancelling Jupyter deployment.\n"
     ]
    }
   ],
   "source": [
    "nb.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-02 22:39:43 INFO: Cancelling job 13969005.\n"
     ]
    }
   ],
   "source": [
    "nodes.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.running()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.run('squeue')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
