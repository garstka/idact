{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Deploying Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will learn how to:\n",
    "\n",
    " - Configure remote Dask.distributed deployment.\n",
    " - Deploy Dask.distributed scheduler and workers on compute nodes.\n",
    " - Access scheduler and worker dashboards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import idact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `idact` to path if it's not already installed, for instance if this notebook is executed in a cloned repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitmath\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a wildcard import for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from idact import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the environment and the cluster. Make sure to use your cluster name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cluster(pro.cyfronet.pl, 22, plggarstka, auth=AuthMethod.PUBLIC_KEY, key='C:\\\\Users\\\\Maciej/.ssh\\\\id_rsa_6p', install_key=False, disable_sshd=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_environment()\n",
    "cluster = show_cluster(\"hpc\")\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_node = cluster.get_access_node()\n",
    "access_node.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure remote Dask deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dask.distributed on the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure `dask`, `distributed` and `bokeh` packages are installed with the Python 3.5+ distribution you intend to use on the cluster, see [Dask documentation](http://distributed.dask.org/en/latest/install.html).\n",
    "\n",
    "If you encounter any problems with deployment, this may be due to some library versions being incompatible. You can try installing frozen versions included with the *idact* repo in `envs/dask_jupyter_tornado.txt`, same as described in the previous tutorial `03. Deploying Jupyter`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify setup actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as for the Jupyter deployment, in order for *idact* to find and execute the proper binaries, you'll need to specify setup steps as a list of Bash script lines.\n",
    "\n",
    "They may very well be the exact same instructions as for the Jupyter deployment. If they are not, replace `cluster.config.setup_actions.jupyter` with the correct instructions list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.config.setup_actions.dask = cluster.config.setup_actions.jupyter\n",
    "save_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the scratch directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask requires a directory for offloading data when the memory starts to fill up. Usually, the faster the storage, the better, see `--local-directory` in [dask-worker documentation](http://distributed.dask.org/en/latest/worker.html#spill-data-to-disk).\n",
    "\n",
    "You can pass an absolute path, or a cluster environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.config.scratch = '$SCRATCH'\n",
    "save_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allocate nodes for the Dask deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will deploy Dask on three nodes: a scheduler on the first node, and one worker on each node. Make sure to adjust the `--account` parameter, same as in previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-24 01:40:34 INFO: Creating the ssh directory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Nodes([Node(NotAllocated),Node(NotAllocated),Node(NotAllocated)], SlurmAllocation(job_id=14334588))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = cluster.allocate_nodes(nodes=3,\n",
    "                               cores=2,\n",
    "                               memory_per_node=bitmath.GiB(10),\n",
    "                               walltime=Walltime(minutes=10),\n",
    "                               native_args={\n",
    "                                   '--account': 'intdata'\n",
    "                               })\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nodes([Node(p0289:60904, 2018-11-24 00:50:41.868727+00:00),Node(p0290:43482, 2018-11-24 00:50:41.868727+00:00),Node(p0291:54153, 2018-11-24 00:50:41.868727+00:00)], SlurmAllocation(job_id=14334588))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.wait()\n",
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the initial setup, Dask can be deployed with a single command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-24 01:40:46 INFO: Deploying Dask on 3 nodes.\n",
      "2018-11-24 01:40:46 INFO: Connecting to p0289:60904 (1/3).\n",
      "2018-11-24 01:40:48 INFO: Connecting to p0290:43482 (2/3).\n",
      "2018-11-24 01:40:50 INFO: Connecting to p0291:54153 (3/3).\n",
      "2018-11-24 01:40:52 INFO: Deploying scheduler on the first node: p0289.\n",
      "2018-11-24 01:41:20 INFO: Checking scheduler connectivity from p0289 (1/3).\n",
      "2018-11-24 01:41:20 INFO: Checking scheduler connectivity from p0290 (2/3).\n",
      "2018-11-24 01:41:20 INFO: Checking scheduler connectivity from p0291 (3/3).\n",
      "2018-11-24 01:41:21 INFO: Deploying workers.\n",
      "2018-11-24 01:41:21 INFO: Deploying worker 1/3.\n",
      "2018-11-24 01:41:42 INFO: Deploying worker 2/3.\n",
      "2018-11-24 01:42:05 INFO: Deploying worker 3/3.\n",
      "2018-11-24 01:42:46 INFO: Retried and failed: config.retries[Retry.OPEN_TUNNEL].{count=3, seconds_between=5}\n",
      "2018-11-24 01:42:46 ERROR: Failure: Adding last hop.\n",
      "2018-11-24 01:43:08 INFO: Validating worker 1/3.\n",
      "2018-11-24 01:43:08 INFO: Validating worker 2/3.\n",
      "2018-11-24 01:43:08 INFO: Validating worker 3/3.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DaskDeployment(scheduler=tcp://localhost:41056/tcp://172.20.65.34:41056, workers=3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = deploy_dask(nodes)\n",
    "dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the deployment fails, take a look at `idact.log` to find out why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a Dask client for the deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://localhost:41056\n",
       "  <li><b>Dashboard: </b><a href='http://localhost:34652/status' target='_blank'>http://localhost:34652/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>3</li>\n",
       "  <li><b>Cores: </b>6</li>\n",
       "  <li><b>Memory: </b>32.21 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://172.20.65.34:41056' processes=3 cores=6>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = dd.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You shouldn't perform computations with Dask.distributed from your local computer, due to likely Python and library version mismatches.\n",
    "\n",
    "Even if your Python environment matched the cluster exactly, the amount of data that could be transferred to your local computer could prove overwhelming.\n",
    "\n",
    "We will address this issue in one of the next notebooks, by making the Dask deployment accessible from a notebook deployed on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Browse Dask dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask provides dashboards for the scheduler and each worker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://localhost:34652/status',\n",
       " 'http://localhost:50707/main',\n",
       " 'http://localhost:43578/main',\n",
       " 'http://localhost:49392/main']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.diagnostics.addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open all dashboards, execute the line below. You can also click the scheduler dashboard link under `get_client` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.diagnostics.open_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancel Dask deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you're done, you can cancel the deployment by calling `cancel`, though it will be killed anyway when the node allocation ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-24 01:43:48 INFO: Cancelling worker deployment on p0291.\n",
      "2018-11-24 01:43:54 INFO: Cancelling worker deployment on p0290.\n",
      "2018-11-24 01:44:02 INFO: Cancelling worker deployment on p0289.\n",
      "2018-11-24 01:44:08 INFO: Cancelling scheduler deployment on p0289.\n"
     ]
    }
   ],
   "source": [
    "dd.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the following will just close the tunnels, without attempting to kill Dask scheduler and workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.cancel_local()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask client is multithreaded, so it needs to be closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancel the allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to cancel an allocation if you're done with it early, in order to minimize the CPU time you are charged for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.running()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-24 01:44:17 INFO: Cancelling job 14334588.\n"
     ]
    }
   ],
   "source": [
    "nodes.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.running()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook, we will install *idact* on the cluster and configure it from a deployed Jupyter Notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
