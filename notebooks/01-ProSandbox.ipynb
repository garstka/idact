{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# idact - Prometheus sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `idact` to path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import bitmath\n",
    "import logging\n",
    "import subprocess\n",
    "\n",
    "sys.path.append('../')  # For running in repo. Unnecessary with pip install.\n",
    "from idact import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hide debug information, setup context manager stack (for testing purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add cluster (only first run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need a new SSH key to connect to the cluster, specify its type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = KeyType.RSA  # Generate a new RSA key (Default location: ~/.ssh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have a key, uncomment this and provide its absolute path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = os.path.expanduser('~/.ssh/id_rsa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you set `install_key` to False, you will not be asked for a password later, but the key must be installed on the cluster manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_key = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 00:20:01 INFO: Generating public-private key pair.\n"
     ]
    }
   ],
   "source": [
    "cluster = add_cluster(name=\"pro\",\n",
    "                      user=\"plggarstka\",\n",
    "                      host=\"pro.cyfronet.pl\",\n",
    "                      port=22,\n",
    "                      auth=AuthMethod.PUBLIC_KEY,\n",
    "                      key=key,\n",
    "                      install_key=install_key,\n",
    "                      scratch=\"$SCRATCH\")\n",
    "save_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cluster (subsequent runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cluster(pro.cyfronet.pl, 22, plggarstka, auth=AuthMethod.PUBLIC_KEY, key='C:\\\\Users\\\\Maciej/.ssh\\\\id_rsa_j7', install_key=True, disable_sshd=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_environment()\n",
    "cluster = show_cluster(\"pro\")\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debug log is saved to `idact.log` for every session if you need to troubleshoot or report bugs, but you can also change log level for messages printed to standard output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_log_level(logging.INFO)\n",
    "save_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Node(pro.cyfronet.pl:22, None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node = cluster.get_access_node()\n",
    "node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On your first action, you may be asked for a password to install the key, if you chosen `install_key=True` while adding the cluster.\n",
    "You can connect explicitly to do this right now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 00:20:07 INFO: Installing key using password authentication.\n",
      "Password for plggarstka@pro.cyfronet.pl:22: \n"
     ]
    }
   ],
   "source": [
    "node.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to save the environment installing the key, so it's installed only once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(cluster.config.install_key)\n",
    "save_environment()  # Never install the key again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run commands on the login node now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plggarstka'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.run('whoami')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'login01.pro.cyfronet.pl'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.run('hostname')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allocate nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 00:20:24 INFO: Creating the ssh directory.\n"
     ]
    }
   ],
   "source": [
    "nodes = cluster.allocate_nodes(nodes=2,\n",
    "                               cores=2,\n",
    "                               memory_per_node=bitmath.GiB(10),\n",
    "                               walltime=Walltime(minutes=20),\n",
    "                               native_args={\n",
    "                                   '--partition': 'plgrid-testing',\n",
    "                                   '--account': 'intdata'\n",
    "                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nodes([Node(NotAllocated),Node(NotAllocated)], SlurmAllocation(job_id=14194860))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nodes([Node(p0097:54199, 2018-11-14 23:40:31.762414+00:00),Node(p0100:33033, 2018-11-14 23:40:31.762414+00:00)], SlurmAllocation(job_id=14194860))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.wait()\n",
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plggarstka'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].run('whoami')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p0097'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].run('hostname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\\n          14194860 plgrid-te     wrap plggarst  R       0:07      2 p[0097,0100]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[1].run('squeue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p0100'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[1].run('hostname')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine node resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GiB(10.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].resources.memory_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GiB(0.0225067138671875)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].resources.memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].resources.cpu_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].resources.cpu_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunnel = nodes[0].tunnel(here=9000, there=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHopTunnel(9000:10000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunnel.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to provide Bash commands that will expose a Python distribution you want work with on the cluster. It should have `idact` installed. Installing `idact` with pip will also install the required `jupyter` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.config.setup_actions.jupyter = ['module load plgrid/tools/python-intel/3.6.2']\n",
    "save_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run Jupyter Notebook on the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JupyterDeployment(8080 -> Node(p0097:54199, 2018-11-14 23:40:31.762414+00:00)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = nodes[0].deploy_notebook(local_port=8080)\n",
    "nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GiB(0.08160018920898438)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].resources.memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8080"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.local_port"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open the deployed notebook server in a new tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.open_in_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GiB(0.08343124389648438)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].resources.memory_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push and pull notebook\n",
    "\n",
    "You can access the deployed notebook from multiple places by first pushing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 00:21:06 INFO: Pushing deployment: JupyterDeployment(8080 -> Node(p0097:54199, 2018-11-14 23:40:31.762414+00:00)\n"
     ]
    }
   ],
   "source": [
    "cluster.push_deployment(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then pulling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 00:21:10 INFO: Pulling deployments.\n",
      "2018-11-15 00:21:13 INFO: Creating the ssh directory.\n",
      "2018-11-15 00:21:21 INFO: Pulled Jupyter deployment: JupyterDeployment(53499 -> Node(p0097:54199, 2018-11-14 23:40:31.762414+00:00)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[JupyterDeployment(53499 -> Node(p0097:54199, 2018-11-14 23:40:31.762414+00:00)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployments = cluster.pull_deployments()\n",
    "deployments.jupyter_deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JupyterDeployment(53499 -> Node(p0097:54199, 2018-11-14 23:40:31.762414+00:00)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_2 = deployments.jupyter_deployments[0]\n",
    "nb_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_2.open_in_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 00:21:26 INFO: Cancelling Jupyter deployment.\n"
     ]
    }
   ],
   "source": [
    "nb_2.cancel()\n",
    "nb.cancel_local()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more information on pushing and pulling deployments in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push and pull nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with Dask, you would usually need a notebook running on the cluster, as deployed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the allocated nodes from the cluster, you need to push their deployment first, same as the notebook deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 00:21:33 INFO: Pushing deployment: Nodes([Node(p0097:54199, 2018-11-14 23:40:31.762414+00:00),Node(p0100:33033, 2018-11-14 23:40:31.762414+00:00)], SlurmAllocation(job_id=14194860))\n"
     ]
    }
   ],
   "source": [
    "cluster.push_deployment(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you would pull the deployment on the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 00:21:38 INFO: Pulling deployments.\n",
      "2018-11-15 00:21:41 INFO: Creating the ssh directory.\n",
      "2018-11-15 00:21:49 INFO: Pulled allocation deployment: Nodes([Node(p0097:54199, 2018-11-14 23:40:31.762414+00:00),Node(p0100:33033, 2018-11-14 23:40:31.762414+00:00)], SlurmAllocation(job_id=14194860))\n",
      "2018-11-15 00:21:49 INFO: Pulled Jupyter deployment: JupyterDeployment(53529 -> Node(p0097:54199, 2018-11-14 23:40:31.762414+00:00)\n",
      "2018-11-15 00:21:57 INFO: Retried and failed: config.retries[Retry.VALIDATE_HTTP_TUNNEL].{count=3, seconds_between=2}\n",
      "2018-11-15 00:21:57 INFO: Discarding a Jupyter deployment, because it is no longer functional: JupyterDeployment(53529 -> Node(p0097:54199, 2018-11-14 23:40:31.762414+00:00).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SynchronizedDeployments(nodes=1, jupyter_deployments=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployments = cluster.pull_deployments()\n",
    "deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nodes([Node(p0097:54199, 2018-11-14 23:40:31.762414+00:00),Node(p0100:33033, 2018-11-14 23:40:31.762414+00:00)], SlurmAllocation(job_id=14194860))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = deployments.nodes[0]\n",
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, this feature is intended for using an allocation in multiple notebooks at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deployments are cleared automatically if they are expired or cancelled. They can also be cleared manually by  running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 00:21:57 INFO: Clearing deployments.\n"
     ]
    }
   ],
   "source": [
    "cluster.clear_pushed_deployments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## idact-notebook app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can deploy nodes and notebook automatically using the following command:\n",
    "```\n",
    "idact-notebook\n",
    "```\n",
    "or:\n",
    "```\n",
    "python -m idact.notebook\n",
    "```\n",
    "Help message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: notebook.py [OPTIONS] CLUSTER_NAME\n",
      "\n",
      "  A console script that executes a Jupyter Notebook instance on an allocated\n",
      "  cluster node, and makes it accessible in the local browser.\n",
      "\n",
      "  CLUSTER_NAME argument is the cluster name to execute the notebook on. It\n",
      "  must already be present in the config file.\n",
      "\n",
      "Options:\n",
      "  -e, --environment TEXT  Environment path. Default: ~/.idact.conf or the\n",
      "                          value of IDACT_CONFIG_PATH.\n",
      "  --save-defaults         Save allocation parameters as defaults for next\n",
      "                          time.\n",
      "  --reset-defaults        Reset unspecified allocation parameters to defaults.\n",
      "  --nodes INTEGER         Cluster node count. [Allocation parameter]. Jupyter\n",
      "                          notebook will be deployed on the first node.\n",
      "                          Default: 1.\n",
      "  --cores INTEGER         CPU core count per node. [Allocation parameter].\n",
      "                          Default: 1\n",
      "  --memory-per-node TEXT  Memory per node. [Allocation parameter]. Default:\n",
      "                          1GiB\n",
      "  --walltime TEXT         Maximum time to allocate the resources for. Format:\n",
      "                          [days-]hours:minutes:seconds. [Allocation\n",
      "                          parameter]. Default: 0:10:00\n",
      "  --native-arg TEXT...    Native arguments for the workload manager. Values\n",
      "                          are not validated. Supported arguments take\n",
      "                          precedence over native arguments. Arguments with\n",
      "                          None as value are treated as flags. Arguments\n",
      "                          specified later override earlier arguments.\n",
      "                          [Allocation parameter]. Default: No native\n",
      "                          arguments.\n",
      "  --help                  Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "help_message = subprocess.getoutput(\n",
    "    \"cd .. && {python} -m idact.notebook --help\".format(\n",
    "        python=sys.executable))\n",
    "print(help_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to deploy a notebook on a cluster with the same parameters as above, you could call:\n",
    "\n",
    "```\n",
    "python -m idact.notebook pro --save-defaults --environment notebooks/.idact-env --nodes 2 --cores 2 --memory-per-node 10GiB --walltime 0:20:00 --native-arg --partition plgrid-testing --native-arg --account intdata\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flag `--save-defaults` is optional, but it saves the allocation parameters: next time, the following will have the same effect:\n",
    "```\n",
    "python -m idact.notebook pro --environment notebooks/.idact-env\n",
    "```\n",
    "The `--environment` argument is optional if you use the default environment location.\n",
    "\n",
    "The allocation and notebook the application deploys can be pulled from the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to provide a list of Bash commands that will expose a Python distribution you want to deploy Dask with. It will likely be the same distribution as for the Jupyter notebook above.\n",
    "\n",
    "Deploying Dask requires `dask`, `distributed`, and `bokeh` on the cluster. If you install `idact` with pip, they will be installed automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.config.setup_actions.dask = ['module load plgrid/tools/python-intel/3.6.2']\n",
    "cluster.config.scratch = '$SCRATCH'\n",
    "save_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 00:21:59 INFO: Deploying Dask on 2 nodes.\n",
      "2018-11-15 00:21:59 INFO: Connecting to p0097:54199 (1/2).\n",
      "2018-11-15 00:21:59 INFO: Connecting to p0100:33033 (2/2).\n",
      "2018-11-15 00:21:59 INFO: Deploying scheduler on the first node: p0097.\n",
      "2018-11-15 00:22:12 INFO: Checking scheduler connectivity from p0097 (1/2).\n",
      "2018-11-15 00:22:12 INFO: Checking scheduler connectivity from p0100 (2/2).\n",
      "2018-11-15 00:22:12 INFO: Deploying workers.\n",
      "2018-11-15 00:22:12 INFO: Deploying worker 1/2.\n",
      "2018-11-15 00:22:23 INFO: Deploying worker 2/2.\n",
      "2018-11-15 00:22:33 INFO: Validating worker 1/2.\n",
      "2018-11-15 00:22:33 INFO: Validating worker 2/2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DaskDeployment(scheduler=tcp://localhost:53541/tcp://172.20.64.97:56230, workers=2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = deploy_dask(nodes)\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GiB(0.2905616760253906)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].resources.memory_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Dask client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://localhost:53541\n",
       "  <li><b>Dashboard: </b><a href='http://localhost:45934/status' target='_blank'>http://localhost:45934/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>21.47 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://172.20.64.97:56230' processes=2 cores=4>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = dd.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].resources.cpu_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation will work only if Python and library versions match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = client.submit(lambda: value + 1, 10)\n",
    "#x.result() == 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagnostics servers are tunnelled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://localhost:45934/status',\n",
       " 'http://localhost:53554/main',\n",
       " 'http://localhost:53562/main']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.diagnostics.addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open diagnostics servers in new tabs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.diagnostics.open_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 00:23:49 INFO: Cancelling worker deployment on p0100.\n",
      "2018-11-15 00:24:00 INFO: Cancelling worker deployment on p0097.\n",
      "2018-11-15 00:24:07 INFO: Cancelling scheduler deployment on p0097.\n"
     ]
    }
   ],
   "source": [
    "dd.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust timeouts\n",
    "\n",
    "Sometimes a timeout occurs during a deployment, and may even cause it to fail. \n",
    "\n",
    "If you find this to happen too often, you may need to adjust the timeouts for your cluster.\n",
    "\n",
    "In order to do that, copy the retry name from the info message preceding the failure that looks similar to this:\n",
    "\n",
    "```\n",
    "2018-11-12 22:14:00 INFO: Retried and failed: config.retries[Retry.PORT_INFO].{count=5, seconds_between=5}\n",
    "```\n",
    "\n",
    "First, you can look up what the current retry config is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetryConfig(count=5, seconds_between=5)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.config.retries[Retry.PORT_INFO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And adjust the retry count and/or seconds between retries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetryConfig(count=6, seconds_between=10)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.config.retries[Retry.PORT_INFO] = set_retry(count=6,\n",
    "                                                    seconds_between=10)\n",
    "cluster.config.retries[Retry.PORT_INFO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetryConfig(count=6, seconds_between=10)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.config.retries[Retry.PORT_INFO].count = 6\n",
    "cluster.config.retries[Retry.PORT_INFO].seconds_between = 10\n",
    "cluster.config.retries[Retry.PORT_INFO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<Retry.PORT_INFO: 0>: RetryConfig(count=5, seconds_between=5),\n",
       " <Retry.JUPYTER_JSON: 1>: RetryConfig(count=5, seconds_between=3),\n",
       " <Retry.SCHEDULER_CONNECT: 2>: RetryConfig(count=5, seconds_between=2),\n",
       " <Retry.DASK_NODE_CONNECT: 3>: RetryConfig(count=3, seconds_between=5),\n",
       " <Retry.DEPLOY_DASK_SCHEDULER: 4>: RetryConfig(count=3, seconds_between=5),\n",
       " <Retry.DEPLOY_DASK_WORKER: 5>: RetryConfig(count=3, seconds_between=5),\n",
       " <Retry.GET_SCHEDULER_ADDRESS: 6>: RetryConfig(count=5, seconds_between=5),\n",
       " <Retry.CHECK_WORKER_STARTED: 7>: RetryConfig(count=5, seconds_between=5),\n",
       " <Retry.CANCEL_DEPLOYMENT: 8>: RetryConfig(count=5, seconds_between=1),\n",
       " <Retry.SQUEUE_AFTER_SBATCH: 9>: RetryConfig(count=3, seconds_between=3),\n",
       " <Retry.OPEN_TUNNEL: 10>: RetryConfig(count=3, seconds_between=5),\n",
       " <Retry.VALIDATE_HTTP_TUNNEL: 11>: RetryConfig(count=3, seconds_between=2)}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_default_retries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.running()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 00:24:31 INFO: Cancelling job 14194860.\n"
     ]
    }
   ],
   "source": [
    "nodes.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.running()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\\n          14194860 plgrid-te     wrap plggarst CG       4:00      2 p[0097,0100]'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.run('squeue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push and pull the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working on a cluster, it may be useful to synchronize idact config with the local machine. Pushing the environment will merge the local environment into the remote environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 00:24:35 INFO: Pushing the environment to cluster.\n",
      "2018-11-15 00:24:37 ERROR: Failure: Getting file from node pro.cyfronet.pl: /net/people/plggarstka/.idact.conf\n",
      "2018-11-15 00:24:37 ERROR: Failure: Deserializing the environment from cluster.\n",
      "2018-11-15 00:24:37 INFO: Remote environment is missing, current environment will be copied to cluster.\n"
     ]
    }
   ],
   "source": [
    "push_environment(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be able to use that environment when working on the remote notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"clusters\": {\n",
      "        \"pro\": {\n",
      "            \"auth\": \"PUBLIC_KEY\",\n",
      "            \"disableSshd\": false,\n",
      "            \"host\": \"pro.cyfronet.pl\",\n",
      "            \"installKey\": true,\n",
      "            \"key\": null,\n",
      "            \"notebookDefaults\": {},\n",
      "            \"port\": 22,\n",
      "            \"retries\": {\n",
      "                \"CANCEL_DEPLOYMENT\": {\n",
      "                    \"count\": 5,\n",
      "                    \"secondsBetween\": 1\n",
      "                },\n",
      "                \"CHECK_WORKER_STARTED\": {\n",
      "                    \"count\": 5,\n",
      "                    \"secondsBetween\": 5\n",
      "                },\n",
      "                \"DASK_NODE_CONNECT\": {\n",
      "                    \"count\": 3,\n",
      "                    \"secondsBetween\": 5\n",
      "                },\n",
      "                \"DEPLOY_DASK_SCHEDULER\": {\n",
      "                    \"count\": 3,\n",
      "                    \"secondsBetween\": 5\n",
      "                },\n",
      "                \"DEPLOY_DASK_WORKER\": {\n",
      "                    \"count\": 3,\n",
      "                    \"secondsBetween\": 5\n",
      "                },\n",
      "                \"GET_SCHEDULER_ADDRESS\": {\n",
      "                    \"count\": 5,\n",
      "                    \"secondsBetween\": 5\n",
      "                },\n",
      "                \"JUPYTER_JSON\": {\n",
      "                    \"count\": 5,\n",
      "                    \"secondsBetween\": 3\n",
      "                },\n",
      "                \"OPEN_TUNNEL\": {\n",
      "                    \"count\": 3,\n",
      "                    \"secondsBetween\": 5\n",
      "                },\n",
      "                \"PORT_INFO\": {\n",
      "                    \"count\": 6,\n",
      "                    \"secondsBetween\": 10\n",
      "                },\n",
      "                \"SCHEDULER_CONNECT\": {\n",
      "                    \"count\": 5,\n",
      "                    \"secondsBetween\": 2\n",
      "                },\n",
      "                \"SQUEUE_AFTER_SBATCH\": {\n",
      "                    \"count\": 3,\n",
      "                    \"secondsBetween\": 3\n",
      "                },\n",
      "                \"VALIDATE_HTTP_TUNNEL\": {\n",
      "                    \"count\": 3,\n",
      "                    \"secondsBetween\": 2\n",
      "                }\n",
      "            },\n",
      "            \"scratch\": \"$SCRATCH\",\n",
      "            \"setupActions\": {\n",
      "                \"dask\": [\n",
      "                    \"module load plgrid/tools/python-intel/3.6.2\"\n",
      "                ],\n",
      "                \"jupyter\": [\n",
      "                    \"module load plgrid/tools/python-intel/3.6.2\"\n",
      "                ]\n",
      "            },\n",
      "            \"user\": \"plggarstka\"\n",
      "        }\n",
      "    },\n",
      "    \"logLevel\": 20\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(node.run('cat ~/.idact.conf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse operation is pulling the environment, which merges the remote environment into the local environment. Machine-specific information like the private key path is skipped when pushing or pulling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 00:24:50 INFO: Pulling the environment from cluster.\n"
     ]
    }
   ],
   "source": [
    "pull_environment(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can remove it if you don't need it for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node.run('rm -v ~/.idact.conf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cluster can be removed from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 00:24:53 INFO: No auth method specified, defaulting to password-based.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Cluster(fakehost, 2222, fakeuser, auth=AuthMethod.ASK, key=None, install_key=True, disable_sshd=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_cluster(name='fake',\n",
    "            user='fakeuser',\n",
    "            host='fakehost',\n",
    "            port=2222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pro': Cluster(pro.cyfronet.pl, 22, plggarstka, auth=AuthMethod.PUBLIC_KEY, key='C:\\\\Users\\\\Maciej/.ssh\\\\id_rsa_j7', install_key=False, disable_sshd=False),\n",
       " 'fake': Cluster(fakehost, 2222, fakeuser, auth=AuthMethod.ASK, key=None, install_key=True, disable_sshd=False)}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cluster('fake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pro': Cluster(pro.cyfronet.pl, 22, plggarstka, auth=AuthMethod.PUBLIC_KEY, key='C:\\\\Users\\\\Maciej/.ssh\\\\id_rsa_j7', install_key=False, disable_sshd=False)}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_clusters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
